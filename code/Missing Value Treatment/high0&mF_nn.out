During startup - Warning message:
Setting LC_CTYPE failed, using "C" 
> library(tidyr)
> library(dplyr)

Attaching package: 'dplyr'

The following objects are masked from 'package:stats':

    filter, lag

The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union

> library(missForest)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> library(randomForest)
randomForest 4.7-1.1
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:dplyr':

    combine

> library('fastDummies')
> library(neuralnet)

Attaching package: 'neuralnet'

The following object is masked from 'package:dplyr':

    compute

> library(keras)
> load("../../data/temp/remData.Rdata")
> 
> r2_calc <- function(classifications, predictions){
+   
+   idx <- !is.na(predictions) & !is.na(classifications)
+   classifications <- classifications[idx]
+   predictions <- predictions[idx]
+   
+   1 - (sum((classifications-predictions)^2)/sum((classifications-mean(classifications))^2))
+   
+ }
> 
> print_dot_callback <- callback_lambda(
+   on_epoch_end = function(epoch, logs) {
+     if (epoch %% 80 == 0) cat("\n")
+     cat(".")
+   }
+ )  
2022-12-01 17:51:02.139117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-01 17:51:07.223983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/sw/pkgs/coe/o/image-libraries/220318/lib:/sw/pkgs/arc/gcc/10.3.0/lib64:/opt/slurm/lib64::
2022-12-01 17:51:07.224002: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-12-01 17:51:07.682777: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-12-01 17:51:23.274643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/sw/pkgs/coe/o/image-libraries/220318/lib:/sw/pkgs/arc/gcc/10.3.0/lib64:/opt/slurm/lib64::
2022-12-01 17:51:23.275878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/sw/pkgs/coe/o/image-libraries/220318/lib:/sw/pkgs/arc/gcc/10.3.0/lib64:/opt/slurm/lib64::
2022-12-01 17:51:23.275887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Loaded Tensorflow version 2.10.1
> 
> df = taksRem %>% select(-contains("_na"))
> 
> df2 = inner_join(df, grdXwalk, by = 'CAMPUS')
> df2 = subset(df2,select = -c(COUNTY,GRDSPAN))
> check = df2 %>% pivot_longer(cols = -c(CAMPUS,GRDTYPE), names_to = c("type","variable"),names_pattern = "(.+)([A-Z]\\d{2})",values_to = "value")
> 
> check = check %>% pivot_wider(names_from = type,values_from = value)
> 
> check2 = check %>% mutate(out = case_when(GRDTYPE %in% c("S", "B") & !is.na(outh) ~ outh,
+                                           GRDTYPE %in% c("M", "E") ~ outm,
+                                           GRDTYPE %in% c("S", "B") & is.na(outh) ~ outm))
> 
> tf = covsRem_noscale %>% select(CAMPUS,matches("_34$"))
> ff = covsRem_noscale %>% select(CAMPUS,matches("_45$"))
> fs = covsRem_noscale %>% select(CAMPUS,matches("_56$"))
> ss = covsRem_noscale %>% select(CAMPUS,matches("_67$"))
> se = covsRem_noscale %>% select(CAMPUS,matches("_78$"))
> 
> name_34 = tf[c(which(rowSums(is.na(tf)) == ncol(tf)-1)),1] # campus name whose column entries are all na == campus not existent in 34
> name_45 = ff[c(which(rowSums(is.na(ff)) == ncol(ff)-1)),1]
> name_56 = fs[c(which(rowSums(is.na(fs)) == ncol(fs)-1)),1]
> name_67 = ss[c(which(rowSums(is.na(ss)) == ncol(ss)-1)),1]
> name_78 = se[c(which(rowSums(is.na(se)) == ncol(se)-1)),1]
> 
> cv_fun <- function(X, y, K=10){
+   N <- nrow(X)
+   folds <- cut(sample(N),breaks=K,labels=FALSE)
+   
+   cv.out <- foreach(k = 1:K, .combine=rbind) %do% {
+     
+     rf <- randomForest(X[(folds != k),], y[(folds != k)],importance = TRUE)
+     rf_preds <- predict(rf, newdata=X[(folds == k),])
+     rf_mse <- mean((rf_preds-y[(folds == k)])^2) 
+     rf_r2 <- r2_calc(y[(folds == k)],rf_preds) 
+     
+     
+     
+     imp = sort(rf$importance[,1]/sum(rf$importance[,1]),decreasing = T)[1:20]
+     
+     train_cov = X[(folds != k),c(names(imp))] 
+     train_target = y[(folds != k)]
+     test_cov = X[(folds == k),c(names(imp))]
+     test_target = y[(folds == k)]
+     
+     covs <- array(data = as.matrix(train_cov), dim = c(nrow(train_cov),1,ncol(train_cov)))
+     response <- array(data = as.matrix(train_target), dim = c(nrow(train_cov), 1))
+     covs_test = array(data = as.matrix(test_cov), dim = c(nrow(test_cov),1,ncol(test_cov)))
+     response_test = array(data = as.matrix(test_target), dim = c(nrow(test_cov), 1))
+     
+     
+     input_layer <- layer_input(shape = c(1,ncol(train_cov)))
+     output_layer <- input_layer %>%
+       layer_masking(mask_value = 0) %>%
+       layer_dropout(rate = .5) %>%
+       layer_lstm(units = 64, return_sequences = F, dropout = .5, recurrent_dropout = .5) %>%
+       layer_dropout(rate = .5) %>%
+       layer_dense(units = 1, activation = "linear")
+     
+     model <- keras_model(input_layer, output_layer)
+     model %>% compile(
+       loss = "mse",
+       #metrics = "accuracy",
+       optimizer = optimizer_adam()
+     )
+     
+     
+     history <- model %>% fit(
+       x = covs,
+       y = response,
+       epochs = 200,
+       validation_split = 0.25,
+       verbose = 0,
+       callbacks = list(print_dot_callback)
+     )
+     
+     pred <- model %>% predict(x = covs_test)
+     error_metric <- mean((pred[,1] - response_test)^2)
+     r2_metric <- r2_calc(response_test,pred[,1])
+     it.ob <- c(error_metric, r2_metric,rf_mse,rf_r2)
+     names(it.ob) <- c("nn_mse","nn_r2","rf_mse","rf_r2")
+     
+     it.ob
+     
+     # error_metric[k] <- mean((pred[,1] - response_test)^2) # 
+     # r2_metric[k] = r2_calc(response_test,pred[,1]) #
+   }
+   
+   return(cv.out)
+ }
> 
> ## middle: replace with 0 ----
> # data_m = check2 %>% filter((!is.na(outm))&(is.na(outh)))
> # data_m = inner_join(data_m, covsRem_noscale, by = 'CAMPUS')
> # data_m = data_m %>% filter(variable == "A08")
> # data_m = merge(data_m, grdXwalk)
> # data_m = dummy_cols(data_m, select_columns = 'GRDSPAN') # 41 levels under GRDSPAN without na in outm
> # data_m = subset(data_m,select = -c(GRDTYPE,Type,variable,outm,outh,GRDSPAN,COUNTY)) # dont need to change out to outm becuz
> # # out is outm, without na as programmed
> # 
> # #start from 3 becuz unlike b4, this time out does not have na
> # for (i in 3:5145){
> #   col = ifelse(is.na(data_m[i]), 1, 0)
> #   data_m[paste0("col",i)] = col
> #   #df$col2 <- ifelse(is.na(df[2]), 1, 0) # 1 = is NA
> # }
> # 
> # data_m$exist34 = ifelse(data_m$CAMPUS %in% name_34,1,0)
> # data_m$exist45 = ifelse(data_m$CAMPUS %in% name_45,1,0)
> # data_m$exist56 = ifelse(data_m$CAMPUS %in% name_56,1,0)
> # data_m$exist67 = ifelse(data_m$CAMPUS %in% name_67,1,0)
> # data_m$exist78 = ifelse(data_m$CAMPUS %in% name_78,1,0)
> # 
> # names(data_m) <- make.names(names(data_m))
> # data_m[,3:5145] = scale(data_m[,3:5145])
> # # replace na with 0
> # data_m[is.na(data_m)] <- 0 # 1394*10321
> # 
> # 
> # 
> # 
> # X = data_m[,3:ncol(data_m)]
> # y = data_m[,2]
> # 
> # X[0:5,0:5]
> # 
> # 
> # start_time <- Sys.time()
> # #
> # middle_zero = cv_fun(X,y,K = 10)
> # 
> # end_time <- Sys.time()
> # end_time - start_time
> # 
> # save(middle_zero,file = 'cv_middle_0.Rdata')
> # 
> # load('cv_middle_0.Rdata')
> # 
> # middle_zero
> # cv_error = mean(middle_zero[,1])
> # r2 = mean(middle_zero[,2])
> 
> # mean(middle_zero[,1])
> # mean(middle_zero[,2])
> 
> 
> 
> ## middle mF ----
> #list.files()
> load("../fullcol.Rdata")
> load("../no_na_middle.Rdata")
> load("../no_na_high.Rdata")
> # middle_df = middle_result$ximp
> # data_m = check2 %>% filter((!is.na(outm))&(is.na(outh)))
> # data_m = inner_join(data_m, covsRem_noscale, by = 'CAMPUS')
> # data_m = data_m %>% filter(variable == "A08")
> # data_m = merge(data_m, grdXwalk)
> # library('fastDummies')
> # data_m = dummy_cols(data_m, select_columns = 'GRDSPAN') # 41 levels under GRDSPAN without na in outm
> # data_m = subset(data_m,select = -c(GRDTYPE,Type,variable,outm,outh,GRDSPAN,COUNTY)) # dont need to change out to outm becuz
> # 
> # for (i in 3:5145){
> #   col = ifelse(is.na(data_m[i]), 1, 0)
> #   data_m[paste0("col",i)] = col
> #   #df$col2 <- ifelse(is.na(df[2]), 1, 0) # 1 = is NA 
> # }
> # #data_m[,5146:10316]# 5146 - 10316 are indicator col
> # 
> # middle_df = cbind(data_m[,5146:10316],middle_df)
> # #middle_df[0,]
> # #middle_df[,5172] # 1 - 5171 col are indicator, where the first 28 are GRDSPAN
> # 
> # middle_df = cbind(data_m$CAMPUS,middle_df)
> # middle_df = cbind(data_m$out,middle_df) # 1394 * 10269
> # 
> # names(middle_df)[names(middle_df) == 'data_m$CAMPUS'] <- 'CAMPUS'
> # names(middle_df)[names(middle_df) == 'data_m$out'] <- 'out'
> # #middle_df = left_join(middle_df, grdXwalk, by = 'CAMPUS') # 1394 * 5101 (5098 + 3 in grdX)
> # middle_df[0,5173:5174] 
> # 
> # #middle_df  = dummy_cols(middle_df, select_columns = 'GRDSPAN') # 28 indicator col
> # #middle_df = subset(middle_df,select = -c(GRDTYPE,GRDSPAN,COUNTY)) # 1394 * 5126
> # 
> # middle_df$exist34 = ifelse(middle_df$CAMPUS %in% name_34,1,0)
> # middle_df$exist45 = ifelse(middle_df$CAMPUS %in% name_45,1,0)
> # middle_df$exist56 = ifelse(middle_df$CAMPUS %in% name_56,1,0)
> # middle_df$exist67 = ifelse(middle_df$CAMPUS %in% name_67,1,0)
> # middle_df$exist78 = ifelse(middle_df$CAMPUS %in% name_78,1,0) # 1394 * 10274
> # 
> # 
> # #middle_df[0,5174:10269] col 5174:10269 are cov cols to scale
> # 
> # middle_df[,5174:10269] = scale(middle_df[,5174:10269])
> # # replace na with 0
> # middle_df[is.na(middle_df)] <- 0
> # names(middle_df) <- make.names(names(middle_df))
> # 
> # # middle_df <- as.matrix(middle_df)
> # # dimnames(middle_df) <- NULL
> # # middle_df = matrix(as.numeric(middle_df),ncol = ncol(middle_df)) # 1394*10274
> # 
> # 
> # 
> # X =middle_df[,3:ncol(middle_df)] # 1394 * 10272
> # y = middle_df[,1] # 1394 * 1
> # 
> # 
> # 
> # start_time <- Sys.time()
> # # 
> # middle_mF= cv_fun(X,y,K = 10)
> # 
> # end_time <- Sys.time()
> # end_time - start_time
> # 
> # save(middle_mF,file = 'cv_middle_mF.Rdata')
> # 
> # load('cv_middle_mF.Rdata')
> # 
> # middle_mF
> # cv_error = mean(middle_mF[,1])
> # r2 = mean(middle_mF[,2])
> # 
> # mean(middle_mF[,1])
> # mean(middle_mF[,2])
> 
> 
> ## high 0 ----
> data_h = check2 %>% filter(!is.na(outh))
> data_h = inner_join(data_h, covsRem_noscale, by = 'CAMPUS')
> data_h = data_h %>% filter(variable == "A08")
> data_h = merge(data_h, grdXwalk)
> data_h = dummy_cols(data_h, select_columns = 'GRDSPAN') 
> data_h = subset(data_h,select = -c(GRDTYPE,Type,variable,outm,outh,GRDSPAN,COUNTY)) 
> #names(which(colSums(is.na(data_h))>0))
> 
> #start from 3 becuz unlike b4, this time out does not have na
> for (i in 3:5145){
+   col = ifelse(is.na(data_h[i]), 1, 0)
+   data_h[paste0("col",i)] = col
+   #df$col2 <- ifelse(is.na(df[2]), 1, 0) # 1 = is NA 
+ }
> 
> data_h$exist34 = ifelse(data_h$CAMPUS %in% name_34,1,0)
> data_h$exist45 = ifelse(data_h$CAMPUS %in% name_45,1,0)
> data_h$exist56 = ifelse(data_h$CAMPUS %in% name_56,1,0)
> data_h$exist67 = ifelse(data_h$CAMPUS %in% name_67,1,0)
> data_h$exist78 = ifelse(data_h$CAMPUS %in% name_78,1,0)
> #names(which(colSums(is.na(data_h))>0)) 
> 
> names(data_h) <- make.names(names(data_h))
> data_h[,3:5145] = scale(data_h[,3:5145])
> # replace na with 0
> data_h[is.na(data_h)] <- 0 # 1474* 10322
> #data_h[0,]
> 
> 
> X = data_h[,3:ncol(data_h)] # 1474 * 10320
> y = data_h[,2] # 1474 * 1
> 
> start_time <- Sys.time()
> #
> high_zero = cv_fun(X,y,K = 10)
2022-12-01 18:07:31.575659: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/usr/local/lib64:/usr/lib/jvm/jre/lib/amd64/server:/sw/pkgs/arc/stacks/gcc/10.3.0/R/4.2.0/lib64/R/lib:/sw/pkgs/coe/o/image-libraries/220318/lib:/sw/pkgs/arc/gcc/10.3.0/lib64:/opt/slurm/lib64::
2022-12-01 18:07:31.576033: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)
2022-12-01 18:07:31.576051: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gl3331.arc-ts.umich.edu): /proc/driver/nvidia/version does not exist
2022-12-01 18:07:31.577057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 1s5/5 [==============================] - 0s 951us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 977us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 911us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 992us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 956us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 996us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 2ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 997us/step
> 
> end_time <- Sys.time()
> end_time - start_time
Time difference of 2.684357 hours
> 
> save(high_zero,file = 'cv_high_0.Rdata')
> 
> load('cv_high_0.Rdata')
> 
> high_zero
            nn_mse     nn_r2    rf_mse     rf_r2
result.1  160.1061 0.7037493 128.05299 0.7630585
result.2  118.8934 0.7123628  92.74258 0.7756290
result.3  142.2323 0.6686068 114.41819 0.7334122
result.4  149.3919 0.6979970 123.01981 0.7513094
result.5  123.5036 0.7282117 104.92067 0.7691062
result.6  149.6623 0.6961092 125.62364 0.7449199
result.7  133.5923 0.6273554 113.17404 0.6843105
result.8  117.1633 0.7274778 107.96507 0.7488729
result.9  195.6762 0.5605716 152.28453 0.6580159
result.10 178.5641 0.6262279 153.71468 0.6782430
> cv_error = mean(high_zero[,1])
> r2 = mean(high_zero[,2])
> 
> mean(high_zero[,1])
[1] 146.8786
> mean(high_zero[,2])
[1] 0.6748669
> 
> ## high_mF ----
> high_df = high_result$ximp # 1474 * 5143 no additional col added
> 
> # getting the campus and out columns back
> data_h = check2 %>% filter(!is.na(outh))
> data_h = inner_join(data_h, covsRem_noscale, by = 'CAMPUS')
> data_h = data_h %>% filter(variable == "A08")
> data_h = merge(data_h, grdXwalk)
> 
> data_h = dummy_cols(data_h, select_columns = 'GRDSPAN') 
> data_h = subset(data_h,select = -c(GRDTYPE,Type,variable,outm,outh,GRDSPAN,COUNTY)) 
> for (i in 3:5145){
+   col = ifelse(is.na(data_h[i]), 1, 0)
+   data_h[paste0("col",i)] = col
+   #df$col2 <- ifelse(is.na(df[2]), 1, 0) # 1 = is NA 
+ }
> data_h[,5146] # 1474*10317
   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [482] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [519] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [667] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [704] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [741] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [778] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1000] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1037] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1074] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1111] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1148] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1185] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1222] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1259] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1296] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0
[1333] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1370] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1407] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[1444] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> 
> high_df = cbind(data_h[,5146:10317],high_df) #1474*10315
> 
> high_df = cbind(data_h$CAMPUS,high_df)
> high_df = cbind(data_h$out,high_df)
> 
> names(high_df)[names(high_df) == 'data_h$CAMPUS'] <- 'CAMPUS'
> names(high_df)[names(high_df) == 'data_h$out'] <- 'out'
> 
> # 1474 * 10317
> 
> high_df$exist34 = ifelse(high_df$CAMPUS %in% name_34,1,0)
> high_df$exist45 = ifelse(high_df$CAMPUS %in% name_45,1,0)
> high_df$exist56 = ifelse(high_df$CAMPUS %in% name_56,1,0)
> high_df$exist67 = ifelse(high_df$CAMPUS %in% name_67,1,0)
> high_df$exist78 = ifelse(high_df$CAMPUS %in% name_78,1,0) # 1474 * 10322
> 
> high_df[0,10317:10322] # 5175 - 10317 are cov col to scale
[1] BPETDISP_78 exist34     exist45     exist56     exist67     exist78    
<0 rows> (or 0-length row.names)
> 
> # scale
> high_df[,5175:10317] = scale(high_df[,5175:10317]) #scaling returns na for zero variance columns: meaning that they are all the same value
> 
> # replace na with 0
> high_df[is.na(high_df)] <- 0
> names(high_df) <- make.names(names(high_df))
> 
> # high_df <- as.matrix(high_df)
> # dimnames(high_df) <- NULL
> # high_df = matrix(as.numeric(high_df),ncol = ncol(high_df)) # 1474*10322
> 
> 
> 
> X =high_df[,3:ncol(high_df)] # 1474 * 10320
> y = high_df[,1] # 1474 * 1
> 
> 
> 
> 
> start_time <- Sys.time()
> # 
> high_mF= cv_fun(X,y,K = 10)

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 964us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 1ms/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 964us/step
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0617s). Check your callbacks.

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 929us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 949us/step

................................................................................
................................................................................
........................................1/5 [=====>........................] - ETA: 0s5/5 [==============================] - 0s 962us/step
> 
> end_time <- Sys.time()
> end_time - start_time
Time difference of 2.770915 hours
> 
> save(high_mF,file = 'cv_high_mF.Rdata')
> 
> load('cv_high_mF.Rdata')
> 
> high_mF
            nn_mse     nn_r2    rf_mse     rf_r2
result.1  118.9744 0.7283751 115.93077 0.7353239
result.2  162.0676 0.7001107 137.82277 0.7449733
result.3  136.7700 0.6852584 133.14234 0.6936066
result.4  155.5027 0.6458494 131.89837 0.6996074
result.5  121.3531 0.7261252 101.11623 0.7717967
result.6  120.0815 0.7258668 115.42314 0.7365013
result.7  120.4221 0.7337546  87.96515 0.8055148
result.8  138.6661 0.6488501 109.43505 0.7228730
result.9  151.3153 0.6373092 132.92422 0.6813911
result.10 213.8270 0.5936240 174.10173 0.6691215
> cv_error = mean(high_mF[,1])
> r2 = mean(high_mF[,2])
> 
> mean(high_mF[,1])
[1] 143.898
> mean(high_mF[,2])
[1] 0.6825124
> 
> 
> proc.time()
    user   system  elapsed 
20204.38   289.28 19718.80 
